{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfe2NTQtLq11"
   },
   "source": [
    "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.57349184, -0.19015688, -0.06584143, -0.86990562, -2.80927706,\n",
       "       -1.43345052,  0.35862361,  0.24627836, -2.25803168, -0.87761289,\n",
       "        2.31023199, -0.3484947 , -2.2575668 , -1.93628665,  1.65242231])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‚Äòconstant‚Äô, ‚Äòinvscaling‚Äô or ‚Äòadaptive‚Äô schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
      "Total training time: 0.10 seconds.\n",
      "Convergence after 10 epochs took 0.10 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
       "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
       "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
       " (1, 15),\n",
       " array([-0.8531383]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1_8bdzitDlM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.  We will be giving you some functions, please write code in that functions only.\n",
    "\n",
    "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
    "\n",
    "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n ‚àí œÉ((w^{(t)})^{T} x_n+b^{t}))- \\frac{Œª}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- œÉ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
    "        $w^{(t+1)}‚Üê w^{(t)}+Œ±(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}‚Üêb^{(t)}+Œ±(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
    "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
    "        you can stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    \n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (dim,1) dimensions\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    w=np.zeros_like(X_train[0])\n",
    "    b=0\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MI5SAjP9ofN"
   },
   "source": [
    "<font color='cyan'>Grader function - 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "def grader_weights(w,b):\n",
    "    assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "    return True\n",
    "grader_weights(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sig= 1/(1 + np.exp(-z))\n",
    "\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YrGDwg3Ae4m"
   },
   "source": [
    "<font color='cyan'>Grader function - 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "    val=sigmoid(z)\n",
    "    assert(val==0.8807970779778823)\n",
    "    return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log10\n",
    "\n",
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    n=len(y_true)\n",
    "    loss=0\n",
    "    for i in range(len(y_true)):\n",
    "        \n",
    "        loss += (-1)*(1/n)*np.sum((y_true[i]* (math.log10( y_pred[i])))+(1-y_true[i])*(math.log10(1-y_pred[i])))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs1BTXVSClBt"
   },
   "source": [
    "<font color='cyan'>Grader function - 3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_logloss(true,pred):\n",
    "    loss=logloss(true,pred)\n",
    "    assert(loss==0.07644900402910389)\n",
    "    return True\n",
    "true=[1,1,0,1,0]\n",
    "pred=[0.9,0.8,0.1,0.8,0.2]\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n ‚àí œÉ((w^{(t)})^{T} x_n+b^{t}))- \\frac{Œª}{N}w^{(t)})$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    z=np.dot(w.T,x) + b \n",
    "    dw= x*(y-sigmoid(z)-(alpha/N)* w)\n",
    "        \n",
    "\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUFLNqL_GER9"
   },
   "source": [
    "<font color='cyan'>Grader function - 4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI3xD8ctGEnJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "    grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "    assert(np.sum(grad_dw)==2.613689585)\n",
    "    return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- œÉ((w^{(t)})^{T} x_n+b^{t}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    " def gradient_db(x,y,w,b):\n",
    "        \n",
    "     #'''In this function, we will compute gradient w.r.to b '''\n",
    "    db = y - sigmoid( (np.matmul(w,x)) + b )\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbcBzufVG6qk"
   },
   "source": [
    "<font color='cyan'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "    grad_db=gradient_db(x,y,w,b)\n",
    "    assert(grad_db==-0.5)\n",
    "    return True\n",
    "\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ùë§(ùë°+1)‚Üêùë§(ùë°)+Œ±(ùëëùë§(ùë°)) \n",
    "\n",
    "ùëè(ùë°+1)‚Üêùëè(ùë°)+Œ±(ùëëùëè(ùë°))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    \n",
    "    \n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implement the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    # for every epoch\n",
    "        # for every data point(X_train,y_train)\n",
    "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "           #compute gradient w.r.to b (call the gradient_db() function)\n",
    "           #update w, b\n",
    "        # predict the output of x_train[for all data points in X_train] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the train loss values in a list\n",
    "        # predict the output of x_test[for all data points in X_test] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the test loss values in a list\n",
    "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
    "    w,b=initialize_weights(X_train)\n",
    "    N=len(X_train)\n",
    "    loss_train=[]\n",
    "    loss_test=[]\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X_train)):\n",
    "            dweight =gradient_dw(X_train[i],y_train[i],w,b,alpha,N)\n",
    "            dbias=gradient_db(X_train[i],y_train[i],w,b)\n",
    "            w = w + eta0 * dweight\n",
    "            b = b + eta0 * dbias\n",
    "        z_train=np.dot(X_train,w)+b\n",
    "        z_test=np.dot(X_test,w)+b\n",
    "        ypred_train= sigmoid(z_train)\n",
    "        ypred_test= sigmoid(z_test)\n",
    "        \n",
    "        loss_train1=logloss(y_train,ypred_train)\n",
    "        loss_test1=logloss(y_test,ypred_test)\n",
    "        loss_train.append(loss_train1)\n",
    "        loss_test.append(loss_test1)\n",
    "    return w,b,loss_train,loss_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUquz7LFEZ6E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=75\n",
    "w,b,train_loss,test_loss=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42979252,  0.19303522, -0.14846993,  0.33809367, -0.22128249,\n",
       "        0.569949  , -0.44518163, -0.08990394,  0.22182953,  0.17382971,\n",
       "        0.19874852, -0.0005843 , -0.08133412,  0.33909013,  0.02298796])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8922527269940091"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1754574843371859,\n",
       " 0.16867157042532582,\n",
       " 0.16639167986531958,\n",
       " 0.16536827532926193,\n",
       " 0.16485707456106383,\n",
       " 0.1645882001023163,\n",
       " 0.16444271321223278,\n",
       " 0.16436263614118496,\n",
       " 0.1643180694530515,\n",
       " 0.16429307373049495,\n",
       " 0.16427897430077493,\n",
       " 0.16427098545161656,\n",
       " 0.16426644190475895,\n",
       " 0.16426384911012898,\n",
       " 0.16426236467945832,\n",
       " 0.1642615119026574,\n",
       " 0.1642610201297372,\n",
       " 0.16426073527355003,\n",
       " 0.1642605693872455,\n",
       " 0.16426047215030304,\n",
       " 0.16426041469604863,\n",
       " 0.1642603804167069,\n",
       " 0.1642603597246379,\n",
       " 0.1642603470619127,\n",
       " 0.1642603391900708,\n",
       " 0.16426033421015734,\n",
       " 0.16426033099990248,\n",
       " 0.16426032888966322,\n",
       " 0.1642603274752386,\n",
       " 0.16426032650929026,\n",
       " 0.1642603258381035,\n",
       " 0.16426032536445398,\n",
       " 0.1642603250256848,\n",
       " 0.1642603247806234,\n",
       " 0.16426032460168455,\n",
       " 0.164260324470028,\n",
       " 0.1642603243725762,\n",
       " 0.16426032430009743,\n",
       " 0.1642603242459896,\n",
       " 0.1642603242054834,\n",
       " 0.1642603241750857,\n",
       " 0.16426032415224126,\n",
       " 0.16426032413505123,\n",
       " 0.16426032412209499,\n",
       " 0.1642603241123321,\n",
       " 0.16426032410496663,\n",
       " 0.16426032409940797,\n",
       " 0.1642603240952125,\n",
       " 0.16426032409204308,\n",
       " 0.16426032408964863,\n",
       " 0.16426032408784247,\n",
       " 0.1642603240864756,\n",
       " 0.16426032408544367,\n",
       " 0.16426032408466645,\n",
       " 0.16426032408407643,\n",
       " 0.1642603240836309,\n",
       " 0.164260324083295,\n",
       " 0.16426032408304053,\n",
       " 0.1642603240828506,\n",
       " 0.16426032408270355,\n",
       " 0.1642603240825944,\n",
       " 0.16426032408251112,\n",
       " 0.16426032408244906,\n",
       " 0.16426032408240224,\n",
       " 0.16426032408236657,\n",
       " 0.16426032408233895,\n",
       " 0.16426032408231786,\n",
       " 0.1642603240823034,\n",
       " 0.16426032408229024,\n",
       " 0.1642603240822825,\n",
       " 0.16426032408227592,\n",
       " 0.16426032408227031,\n",
       " 0.16426032408226723,\n",
       " 0.164260324082264,\n",
       " 0.16426032408226254]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1759547441481585,\n",
       " 0.16939931352785859,\n",
       " 0.16720591191296946,\n",
       " 0.16621717797712324,\n",
       " 0.1657195946374875,\n",
       " 0.1654555709626687,\n",
       " 0.16531135022280097,\n",
       " 0.16523116855206854,\n",
       " 0.16518605900901984,\n",
       " 0.1651604565463819,\n",
       " 0.16514582031758648,\n",
       " 0.16513739838632227,\n",
       " 0.16513252087837477,\n",
       " 0.16512967662910438,\n",
       " 0.16512800522364235,\n",
       " 0.1651270142478074,\n",
       " 0.16512642053988932,\n",
       " 0.1651260604779014,\n",
       " 0.16512583901798344,\n",
       " 0.16512570062122825,\n",
       " 0.16512561260498806,\n",
       " 0.16512555557494782,\n",
       " 0.16512551790645463,\n",
       " 0.16512549254869247,\n",
       " 0.16512547516554432,\n",
       " 0.16512546304812392,\n",
       " 0.16512545447445603,\n",
       " 0.16512544832940512,\n",
       " 0.16512544387685932,\n",
       " 0.16512544062157467,\n",
       " 0.16512543822424977,\n",
       " 0.16512543644848246,\n",
       " 0.16512543512708228,\n",
       " 0.1651254341402637,\n",
       " 0.16512543340126087,\n",
       " 0.16512543284665082,\n",
       " 0.16512543242974131,\n",
       " 0.16512543211594963,\n",
       " 0.16512543187954207,\n",
       " 0.16512543170130334,\n",
       " 0.1651254315668478,\n",
       " 0.16512543146537612,\n",
       " 0.16512543138877267,\n",
       " 0.1651254313309283,\n",
       " 0.16512543128724275,\n",
       " 0.1651254312542426,\n",
       " 0.1651254312293167,\n",
       " 0.16512543121048243,\n",
       " 0.16512543119625356,\n",
       " 0.16512543118550185,\n",
       " 0.16512543117737916,\n",
       " 0.1651254311712407,\n",
       " 0.1651254311666023,\n",
       " 0.16512543116309647,\n",
       " 0.16512543116044787,\n",
       " 0.16512543115844633,\n",
       " 0.1651254311569348,\n",
       " 0.16512543115579245,\n",
       " 0.16512543115492825,\n",
       " 0.16512543115427494,\n",
       " 0.16512543115378228,\n",
       " 0.16512543115340966,\n",
       " 0.16512543115312822,\n",
       " 0.16512543115291567,\n",
       " 0.16512543115275427,\n",
       " 0.1651254311526331,\n",
       " 0.16512543115254108,\n",
       " 0.16512543115247136,\n",
       " 0.1651254311524185,\n",
       " 0.16512543115237885,\n",
       " 0.16512543115234904,\n",
       " 0.16512543115232567,\n",
       " 0.1651254311523097,\n",
       " 0.16512543115229633,\n",
       " 0.16512543115228726]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "<font color='red'>Goal of assignment</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx8Rs9rfEZ1R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00642561,  0.00755957,  0.00012042, -0.0033504 , -0.01309579,\n",
       "          0.00978321,  0.00724319,  0.00418419,  0.01255633, -0.00701155,\n",
       "          0.00169661, -0.00480345, -0.00173043,  0.00056212,  0.00032075]]),\n",
       " array([-0.03911443]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "w-clf.coef_, b-clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
    "\n",
    "* epoch number on X-axis\n",
    "* loss on Y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUN8puFoEZtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6978933333333335\n",
      "1.6986400000000001\n"
     ]
    }
   ],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z= np.dot(w,X[i]) + b\n",
    "        res=sigmoid(z)\n",
    "        if res.any() >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "\n",
    "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-k28U1xDsLIO"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmYXVWZ7/Hv7wxVJzNkIAkJmECYBQMmAcGAqGiCNqAMgqIEpbHvlRa7Lyjcbu2G1m76OqE2owrOIoPBqKFBkaFxgCQYNAOYEAMpAiQkZE5VUlXv/WPvCieVU1WnhpNzQn6f56nn7HHtd1cl9dZaa++1FBGYmZn1VKbaAZiZ2Z7NicTMzHrFicTMzHrFicTMzHrFicTMzHrFicTMzHrFicT2KpJulvTZGohjhqTHqh0HgKSQNKHacdiey4nE9hiSlkt6Z2/KiIi/i4h/66uYKkHSuPSXe64PyvqOpM/3RVxmHXEisdeNvvjFa2bd50RiewRJ3wcOBH4uaZOkTxf95f4xSc8Dv0mPvUvSS5LWS3pU0lFF5ez4C13S2yQ1SPo/klZJelHSxZ3EcLGkxZI2Slom6eNF+zotS9IwSbMkbZD0BHBwJ7f7aPq5Lr3Xt6RlfDS9/quS7pf0hnS7JH01ve56SX+S9EZJlwIfAj6dlvPzMr7PQyR9T9JqSc9J+mdJmXTfBEmPpNd4RdJPOrt+V9ey1w//BWd7hIj4sKSpwCUR8WtImoDS3acARwCt6fp9wEeBbcB/Aj8EJnZQ9ChgCDAGOA24W9K9EfFqiWNXAe8FlgEnA/dJmhMRT5ZR1g1AIzAaGA/cD/y1g5hOTvftExHN6b2eBfxf4G+AJcBVwI+BE4F3peccCqwHDgfWRcStkk4EGiLinzu4VnvfSO/hIGAY8ADwIvBt4N/S9VOBOmBSek7J65d5PXsdcI3EXg/+NSI2R8RWgIi4LSI2RkQT8K/AmyQN6eDc7cC1EbE9ImYDm4DDSh0YEb+MiGcj8QjJL9WpXZUlKQucDXwujXMB8N1u3uPHgf+IiMVpcvl3YGJaK9kODCL5Ba70mBe7WT5pnB8Ark6/f8uBLwMfLrq/NwD7R0RjRDxWtL3X17c9lxOJvR6saFuQlJV0naRnJW0Alqe7hndw7pq2v/pTW4CBpQ6UNF3SHyStlbQOOL1duR2VNYKk9r+iaN9zZdxXsTcAX5O0Lr32WkDAmIj4DfBfJLWelyXdKmlwN8uH5F7q2sX2HEkNC+DT6TWfkLRQ0kcB+vD6todyIrE9SUdDVRdv/yBwJvBOkiaacel29ebCkuqBe4AvASMjYh9gdpnlrgaagQOKth3YyfGl7nMF8PGI2Kfoq19E/A4gIr4eEW8GjiJpYrqyk7I68gqv1TqK43whvcZLEfG3EbE/SQ3pxrbHhju5vu0FnEhsT/IySdt9ZwYBTcAaoD9JE1BfqAPqSZOCpOkkfQNdiogW4KfAv0rqL+lI4KJOTllN0t9TfK83A1e3PTiQdoqfmy5PlnS8pDywmaQvpiU9r5zvWXGcdwJfkDQobTb7R+AH6XXOlTQ2PfxVkiTV0sX1bS/gRGJ7kv8A/jlt3rmig2O+R9Ic8wKwCPhDX1w4IjYCnyT5RfsqSc1nVjeKuIykmesl4DvA7Z1cawvwBeC36b2eEBEzSR4cuCNtslsATE9PGQx8M43rOZIk+qV037eBI9Ny7i0jzr8nSQbLgMeAHwG3pfsmA49L2kRy75dHxF+7uL7tBeSJrczMrDdcIzEzs15xIjEzs15xIjEzs15xIjEzs17ZK4ZIGT58eIwbN67aYZiZ7VHmzZv3SkSM6Oq4vSKRjBs3jrlz51Y7DDOzPYqkskZgcNOWmZn1ihOJmZn1ihOJmZn1yl7RR2Jmrz/bt2+noaGBxsbGaoeyxysUCowdO5Z8Pt+j8yuaSCRNA74GZIFvRcR17fafDFwPHAOcHxF3p9tPBb5adOjh6f57JQn4PHAuycBwN0XE1yt5H2ZWexoaGhg0aBDjxo0j+bVgPRERrFmzhoaGBsaPH9+jMiqWSNJJcm4gmSmuAZgjaVZELCo67HlgBrDTAHwR8RDpjHaShgJLSSYRIj3+AODwiGiVtF+l7sHMaldjY6OTSB+QxLBhw1i9enWPy6hkjWQKsDQilgFIuoNknogdiSSdgQ1JraUKSJ0D3JeOiArwv4APRkRrWsaqvg/dzPYETiJ9o7ffx0p2to9h5xnhGnhtprXuOJ9kbuo2BwMfkDRX0n2SDil1kqRL02Pm9jjTPvUTmPPtnp1rZraXqGQiKZXiujVmvaTRwNHA/UWb64HGiJhEMgfCbaXOjYhbI2JSREwaMaLLFzNLW/hTmPednp1rZraXqGQiaWDnqUXHAiu7WcZ5wMyI2N6u3HvS5ZkkHfWVkStAs58IMbNdrVu3jhtvvLHb551++umsW7eu2+fNmDGDu+++u9vn7Q6VTCRzgEMkjZdUR9JE1Z0Z5QAuYOdmLYB7gbeny6cAf+lVlJ3J94ftWytWvJntuTpKJC0tnc8yPHv2bPbZZ59KhVUVFetsj4hmSZeRNEtlgdsiYqGka4G5ETFL0mSSWsW+wN9IuiYi2uakHkdSo3mkXdHXAT+U9A/AJuCSSt0D+YITidke4JqfL2TRyg19WuaR+w/mX/7mqA73X3XVVTz77LNMnDiRfD7PwIEDGT16NPPnz2fRokWcddZZrFixgsbGRi6//HIuvfRS4LWx/zZt2sT06dN561vfyu9+9zvGjBnDz372M/r169dlbA8++CBXXHEFzc3NTJ48mZtuuon6+nquuuoqZs2aRS6X413vehdf+tKXuOuuu7jmmmvIZrMMGTKERx99tM++R20q+h5JRMwGZrfb9rmi5TkkTV6lzl1Oic75iFgHvKdPA+2IayRm1oHrrruOBQsWMH/+fB5++GHe8573sGDBgh3vYtx2220MHTqUrVu3MnnyZM4++2yGDRu2UxlLlizhxz/+Md/85jc577zzuOeee7jwwgs7vW5jYyMzZszgwQcf5NBDD+UjH/kIN910Ex/5yEeYOXMmTz/9NJJ2NJ9de+213H///YwZM6ZHTWrl8JvtnckVoHkrRIAfMzSrWZ3VHHaXKVOm7PRC39e//nVmzpwJwIoVK1iyZMkuiWT8+PFMnDgRgDe/+c0sX768y+s888wzjB8/nkMPPRSAiy66iBtuuIHLLruMQqHAJZdcwnve8x7e+973AnDSSScxY8YMzjvvPN7//vf3xa3uwmNtdSbfD6IVWrZVOxIzq3EDBgzYsfzwww/z61//mt///vc89dRTHHvssSWHcqmvr9+xnM1maW5u7vI6EaUffs3lcjzxxBOcffbZ3HvvvUybNg2Am2++mc9//vOsWLGCiRMnsmbNmu7eWpdcI+nE1qijHyTNW7n6rg43s73IoEGD2LhxY8l969evZ99996V///48/fTT/OEPf+iz6x5++OEsX76cpUuXMmHCBL7//e9zyimnsGnTJrZs2cLpp5/OCSecwIQJEwB49tlnOf744zn++OP5+c9/zooVK3apGfWWE0kn7pj/ChdDkkj6vb6esjCz3hk2bBgnnXQSb3zjG+nXrx8jR47csW/atGncfPPNHHPMMRx22GGccMIJfXbdQqHA7bffzrnnnrujs/3v/u7vWLt2LWeeeSaNjY1EBF/9ajJc4ZVXXsmSJUuICN7xjnfwpje9qc9iaaOOqkmvJ5MmTYqezJB4+43/zsWr/hM++UcYelAFIjOznlq8eDFHHHFEtcN43Sj1/ZQ0L335u1PuI+lE5NLH8PzklplZh9y01Qnl2xKJ3243s93jE5/4BL/97W932nb55Zdz8cUXVymirjmRdGZHItnS+XFmZn3khhtuqHYI3eamrU5k6vonCx5vy8ysQ04knVCaSFq3ba5yJGZmtcuJpBO5NJE0N7mz3cysI04kncjWJ30k2xtdIzEz64gTSSdy9cmQB81N7mw3s531dD4SgOuvv54tWzr/vTJu3DheeeWVHpW/uzmRdCJXSJq2WppcIzGznVU6kexJ/PhvJ+rq+tEaomWb+0jMatp9V8FLf+7bMkcdDdOv63B38Xwkp512Gvvttx933nknTU1NvO997+Oaa65h8+bNnHfeeTQ0NNDS0sJnP/tZXn75ZVauXMmpp57K8OHDeeihh7oM5Stf+Qq33ZbMKn7JJZfwqU99qmTZH/jAB0rOSVJpTiSdKNTnaKSO1m2vn78czKxvFM9H8sADD3D33XfzxBNPEBGcccYZPProo6xevZr999+fX/7yl0AymOOQIUP4yle+wkMPPcTw4cO7vM68efO4/fbbefzxx4kIjj/+eE455RSWLVu2S9lr164tOSdJpTmRdKKQy9JInlbXSMxqWyc1h93hgQce4IEHHuDYY48FYNOmTSxZsoSpU6dyxRVX8JnPfIb3vve9TJ06tdtlP/bYY7zvfe/bMUz9+9//fv7nf/6HadOm7VJ2c3NzyTlJKs19JJ0o5DNspd5vtptZpyKCq6++mvnz5zN//nyWLl3Kxz72MQ499FDmzZvH0UcfzdVXX821117bo7JLKVV2R3OSVJoTSScK+SyNUUd4rC0za6d4PpJ3v/vd3HbbbWzatAmAF154gVWrVrFy5Ur69+/PhRdeyBVXXMGTTz65y7ldOfnkk7n33nvZsmULmzdvZubMmUydOrVk2Zs2bWL9+vWcfvrpXH/99cyfP78yN9+Om7Y6Uchn2UId/V0jMbN2iucjmT59Oh/84Ad5y1veAsDAgQP5wQ9+wNKlS7nyyivJZDLk83luuukmAC699FKmT5/O6NGju+xsP+6445gxYwZTpkwBks72Y489lvvvv3+Xsjdu3FhyTpJK83wknXhx/VZWfnkq+48Yxui/v78CkZlZT3k+kr7l+UgqpF8+y9aoI9PsznYzs464aasThXyWRurItLhpy8wq4/jjj6epqWmnbd///vc5+uijqxRR9zmRdKI+l6GRerIta6sdipmVEBFIqnYYvfL4449XO4QOnwwrl5u2OiGJbaoj29LU9cFmtlsVCgXWrFnT61+Ce7uIYM2aNRQKhR6XUdEaiaRpwNeALPCtiLiu3f6TgeuBY4DzI+LudPupQPHjBoen++8tOvcbwMURMbCS97A9UyDX6sd/zWrN2LFjaWhoYPXq1dUOZY9XKBQYO3Zsj8+vWCKRlAVuAE4DGoA5kmZFxKKiw54HZgBXFJ8bEQ8BE9NyhgJLgQeKyp4E7FOp2Is1Z+rJtbpGYlZr8vk848ePr3YYRmWbtqYASyNiWURsA+4Aziw+ICKWR8SfgNZOyjkHuC8itsCOBPVF4NOVCXtnLdkC+dZGcPXZzKykSiaSMcCKovWGdFt3nQ/8uGj9MmBWRLzY2UmSLpU0V9Lc3lR9W7IFMgS0bOtxGWZmr2eVTCSlHqXo1p/1kkYDRwP3p+v7A+cC3+jq3Ii4NSImRcSkESNGdOeyO2nJJrMkerwtM7PSKplIGoADitbHAiu7WcZ5wMyI2J6uHwtMAJZKWg70l7S0t4F2JrL1yYLH2zIzK6mST23NAQ6RNB54gaSJ6oPdLOMC4Oq2lYj4JTCqbV3SpoiY0Aexdqg17xqJmVlnKlYjiYhmkv6M+4HFwJ0RsVDStZLOAJA0WVIDSXPVLZIWtp0vaRxJjeaRSsVYDuXSRNLsGomZWSkVfY8kImYDs9tt+1zR8hySJq9S5y6ni875Sr9DAhA7aiQeb8vMrBS/2d4FOZGYmXXKiaQLquufLDiRmJmV5ETSBdWl4894KHkzs5KcSLqQyQ8AoHWbn9oyMyvFiaQL+ULStLW9yYnEzKwUJ5IuZNM+kubGzVWOxMysNjmRdCFXnySSlib3kZiZleJE0oW6+n60hmje5hqJmVkpTiRdKNQl87a3bnONxMysFCeSLhTyWbZS56e2zMw64ETShUI+qZHgGomZWUlOJF0o5LM0Rh3h0X/NzEpyIulCIZ9JaiSej8TMrCQnki4Uclm2Uo88RIqZWUlOJF1ImrbyyPORmJmV5ETShX75pEaScY3EzKwkJ5Iu1OczNFFHpsU1EjOzUpxIulCfy7CVOrJOJGZmJTmRdEES21VPtrWp2qGYmdUkJ5IybM8UyLe6RmJmVooTSRlaMvXkWpsgotqhmJnVHCeSMrTk+pEhoNnNW2Zm7TmRlKElU58s+BFgM7NdOJGUoTXXL1nY7kRiZtaeE0kZIpfWSJxIzMx2UdFEImmapGckLZV0VYn9J0t6UlKzpHOKtp8qaX7RV6Oks9J9P0zLXCDpNkn5St4DQOSS6XadSMzMdlWxRCIpC9wATAeOBC6QdGS7w54HZgA/Kt4YEQ9FxMSImAi8HdgCPJDu/iFwOHA00A+4pFL3sEO+kHx6vC0zs13kKlj2FGBpRCwDkHQHcCawqO2AiFie7mvtpJxzgPsiYkt6zuy2HZKeAMb2eeTt5dtqJJ6TxMysvUo2bY0BVhStN6Tbuut84MftN6ZNWh8G/rvUSZIulTRX0tzVq1f34LKvydSlNRLPSWJmtotKJhKV2NatN/okjSZpwrq/xO4bgUcj4n9KnRsRt0bEpIiYNGLEiO5cdlf5AcmnayRmZruoZNNWA3BA0fpYYGU3yzgPmBkR24s3SvoXYATw8V5FWKZMXfr4r/tIzMx2UckayRzgEEnjJdWRNFHN6mYZF9CuWUvSJcC7gQsiorO+lT6TrUv6SFq3uUZiZtZexRJJRDQDl5E0Sy0G7oyIhZKulXQGgKTJkhqAc4FbJC1sO1/SOJIazSPtir4ZGAn8Pn00+HOVuoc2ufokkWxvdCIxM2uvkk1bbU9YzW637XNFy3Po4Kmr9ImuXTrnI6KiMZeSKyR9JC1Nm3f3pc3Map7fbC9DXV09rSGa3bRlZrYLJ5IyFOpybKWOliYnEjOz9nZ7M9GeqD6XpZE62OYhUszM2nONpAz96rJspZ7weyRmZrtwIilDIZehKfIetNHMrAQnkjIU8m01EicSM7P2nEjKUMinfSROJGZmu3AiKUMhn2Fr1JHxEClmZrtwIilDW41ETiRmZrtwIilDWyLJtLhpy8ysPSeSMhTyGRqjjmyLayRmZu05kZShLpuhUfVOJGZmJTiRlEES21VPrrWp2qGYmdUcJ5IyNWfqqWtthOjWJI9mZq97TiRlasmm87Y3u1ZiZlbMiaRMLdl0ul2Pt2VmthMnkjK15tpqJO5wNzMr5kRSph2JxMOkmJntpKxEIulgSfXp8tskfVLSPpUNrbZE1onEzKyUcmsk9wAtkiYA3wbGAz+qWFQ1KPJtfSROJGZmxcpNJK0R0Qy8D7g+Iv4BGF25sGqP2hJJsxOJmVmxchPJdkkXABcBv0i35SsTUm1Svn+y4BqJmdlOyk0kFwNvAb4QEX+VNB74QeXCqkFu2jIzKylXzkERsQj4JICkfYFBEXFdJQOrNZl6JxIzs1LKfWrrYUmDJQ0FngJul/SVyoZWW7J1adOW+0jMzHZSbtPWkIjYALwfuD0i3gy8s6uTJE2T9IykpZKuKrH/ZElPSmqWdE7R9lMlzS/6apR0VrpvvKTHJS2R9BNJdWXeQ69k6gYA0NLkN9vNzIqVm0hykkYD5/FaZ3unJGWBG4DpwJHABZKObHfY88AM2j1KHBEPRcTEiJgIvB3YAjyQ7v5P4KsRcQjwKvCxMu+hV/KFpGmreZsTiZlZsXITybXA/cCzETFH0kHAki7OmQIsjYhlEbENuAM4s/iAiFgeEX8CWjsp5xzgvojYIkkkieXudN93gbPKvIdeqasr0BKiudGJxMysWLmd7XcBdxWtLwPO7uK0McCKovUG4PjuBgicD7T1xwwD1qXvtLSVOabUSZIuBS4FOPDAA3tw2Z0V8jkaqaPVNRIzs52U29k+VtJMSaskvSzpHkljuzqtxLZuTeaRNqcdTVIb6laZEXFrREyKiEkjRozozmVLqs9n2Eq9E4mZWTvlNm3dDswC9iepAfw83daZBuCAovWxwMpuxnceMDMitqfrrwD7SGqrSfWkzB4p5LOukZiZlVBuIhkREbdHRHP69R2gqz/z5wCHpE9Z1ZE0Uc3qZnwXAD9uW4mIAB4i6TeB5E37n3WzzB7pl8/SFHnC75GYme2k3ETyiqQLJWXTrwuBNZ2dkPZjXEbSLLUYuDMiFkq6VtIZAJImS2oAzgVukbSw7XxJ40hqNI+0K/ozwD9KWkrSZ/LtMu+hVwr5LFuph21OJGZmxcrqbAc+CvwX8FWSPonfkQyb0qmImA3Mbrftc0XLc0iap0qdu5wSHelpR/+UMuPuM4V8hkbqCL+QaGa2k7JqJBHxfEScEREjImK/iDiL5OXEvUYhn2Vr1CE3bZmZ7aQ3MyT+Y59FsQco5JLO9kyLp9o1MyvWm0RS6lHc1622pq2M52w3M9tJbxJJt94J2dMV6rI0Rh3ZFjdtmZkV67SzXdJGSicMAf0qElGNKuSyrGMgddvXQwRor6qQmZl1qNNEEhGDdlcgtS6fFS8zlHxrE2x9FfoPrXZIZmY1oTdNW3sVSazJDE9WNrxQ3WDMzGqIE0k3rM3tlyxs2C2jspiZ7RGcSLphfT4dFWZ9Q3UDMTOrIU4k3bApP4wWMq6RmJkVcSLphrp8Heuzw5xIzMyKOJF0QyGfYU12OGxw05aZWRsnkm4o5LOs1nDXSMzMijiRdEMhn+VlhiaJJPaqF/vNzDrkRNIN/fJZXmQYbN+SvJRoZmZOJN1Rn8+wsjV9o93NW2ZmgBNJtwyoy7F8+z7Jit9uNzMDnEi6ZeTgepY2DklWnEjMzAAnkm4ZObjAavYh5JcSzczaOJF0w+gh/Wghy7Z+I2G9ayRmZuBE0i2jhtQDsLl+PzdtmZmlnEi6YeTgAgCv5ka4acvMLOVE0g2DCnkG1udYpWFJjcQvJZqZOZF018jB9TS0DE1eSmxcV+1wzMyqzomkm0YNKfDXtndJ3OFuZlbZRCJpmqRnJC2VdFWJ/SdLelJSs6Rz2u07UNIDkhZLWiRpXLr9Hek58yU9JmlCJe+hvVGD+7Fk6+Bkxf0kZmaVSySSssANwHTgSOACSUe2O+x5YAbwoxJFfA/4YkQcAUwBVqXbbwI+FBET0/P+ue+j79ioIfUs3jwoWfGTW2Zm5CpY9hRgaUQsA5B0B3AmsKjtgIhYnu5rLT4xTTi5iPhVetymot0BpFUChgC7tVowanCBF1uHEMogJxIzs4omkjHAiqL1BuD4Ms89FFgn6afAeODXwFUR0QJcAsyWtBXYAJxQqgBJlwKXAhx44IE9uoFSRqUvJW7vtx91btoyM6toH4lKbCv3edkcMBW4ApgMHETSBAbwD8DpETEWuB34SqkCIuLWiJgUEZNGjBjRnbg7NSp9l2RLYZSbtszMqGwiaQAOKFofS/nNUA3AHyNiWUQ0A/cCx0kaAbwpIh5Pj/sJcGJfBVyOkenb7evyI/zUlpkZlU0kc4BDJI2XVAecD8zqxrn7pokD4O0kfSuvAkMkHZpuPw1Y3Icxd2n4gHpyGbFawzxTopkZFUwkaU3iMuB+kl/2d0bEQknXSjoDQNJkSQ3AucAtkham57aQNGs9KOnPJM1k30zL/FvgHklPAR8GrqzUPZSSyYiRgwu80DoUtm/2S4lmtterZGc7ETEbmN1u2+eKlueQNHmVOvdXwDElts8EZvZtpN0zcnA9z+2Y4Gol9Nu3muGYmVWV32zvgdFD+rFkxwRXfnLLzPZuTiQ9MHJwgUWbByYr6xuqG4yZWZU5kfTAqCH1PLdtsGdKNDPDiaRH2l5KbO6/nxOJme31nEh6oO2lxK2FkbDBTVtmtndzIumBtkSyPj/SNRIz2+s5kfTAfoOTt9tfyQxL3m73S4lmthdzIumBQj7L0AF1PK/9k5cSX11e7ZDMzKrGiaSHRg4uMDcOT1ae+211gzEzqyInkh4aPaTAk1tGQr+hsNyJxMz2Xk4kPTRycIGXNm6DN5wIzz1W7XDMzKrGiaSHRg0usGbzNpoPPBHWPQ/rVnR9kpnZ65ATSQ+NHpI8AvzKsMnJBveTmNleyomkh0amiWRFfjwUhsByN2+Z2d7JiaSH2l5KfHHjdjjwRNdIzGyv5UTSQ6PSGsnL6xth3EmwdhlseLHKUZmZ7X5OJD00uJCjXz7LSxsa4Q0nJRtdKzGzvZATSQ9JYtSQAi+tb4RRx0DdIPeTmNleyYmkF0YNLiQ1kmwODjzBNRIz2ys5kfTCjhoJJP0kr/wFNq2qblBmZruZE0kvjBxc4OUNjbS0BrzhrclG10rMbC/jRNILh40aSHNr8PRLG2D/iZAf4HG3zGyv40TSC5PHDQVgzl/XQjYPB0xxjcTM9jpOJL0wZp9+jB5SYM7yV5MN406CVYtg40vVDczMbDeqaCKRNE3SM5KWSrqqxP6TJT0pqVnSOe32HSjpAUmLJS2SNC7dLklfkPSXdN8nK3kPnZHE5HFDmbN8LREBR70flIHf31CtkMzMdruKJRJJWeAGYDpwJHCBpCPbHfY8MAP4UYkivgd8MSKOAKYAbY9DzQAOAA5P993R58F3w+TxQ1m1sYnn126BYQfDG8+GOd+GzWuqGZaZ2W5TyRrJFGBpRCyLiG0kv/DPLD4gIpZHxJ+A1uLtacLJRcSv0uM2RcSWdPf/Aq6NiNZ0X1Wft52S9pM88de1yYapV8D2LfCHG6sYlZnZ7lPJRDIGKJ6koyHdVo5DgXWSfirpj5K+mNZwAA4GPiBprqT7JB3ShzF32yH7DWRIvzxzlqeJZL/D4cgz4IlbYeur1QzNzGy3qGQiUYltUea5OWAqcAUwGTiIpEkLoB5ojIhJwDeB20peXLo0TTZzV69e3Z24uyWTEZPH7ftahzvAyVdC0wZ4/JaKXdfMrFZUMpE0kPRltBkLrOzGuX9Mm8WagXuB44r23ZMuzwSOKVVARNwaEZMiYtKIESO6HXx3TB43lL++splVG9O33EcdDYe9J2neatxQ0WubmVVbJRPJHOAQSeMl1QHnA7O6ce6+ktoywNuBRenyvek6wCnAX/oo3h6blPaTzCuulZxyJTSuhznfrFJUZma7R8USSVqTuAy4H1gM3BkRCyVEBpxzAAAMzUlEQVRdK+kMAEmTJTUA5wK3SFqYnttC0qz1oKQ/kzSTtf1Gvg44O93+H8AllbqHch09ZgiFfIYn2vpJAPY/FiacljwK3LSxesGZmVWYIsrttthzTZo0KebOnVvRa5x/6+/Z1NTML/5+6msbG+bBt98JB70NLvgJ5OoqGoOZWV+SNC/tj+6U32zvI1PGDWXRyg1sbNz+2saxb4YzvgHP/gZmfhxaW6oXoJlZhTiR9JHJ44fSGvDk8+t23nHshXDav8HCn8LsK2AvqAGa2d7FiaSPHHfgvmQzSgZwbO+kT8JJn4K5t8FD/777gzMzq6BctQN4vRhQn+Oo/Qe/9mJie+/8V9iyBh79f7DueTjtGhg0aneGaGZWEa6R9KHJ44Yyf8U6mppL9IVI8DdfS4ZQWfhT+MYk+N1/Qcv2XY81M9uDOJH0oamHDKepuZXZf36x9AGZLLzjs/C//5DM8f7AP8FNJ8GT34MtHdRkzMxqnBNJHzr5kBEcNnIQNz70LK2tnXSqDzsYPnQXXHAHRCvM+nv44gT43lkw7zuwdhm0tnZ8vplZDXEfSR/KZMT/PvVgLr9jPr9a/DLvPqqTPhAJDpsOh06DF5+CRT+DRffCzy9P9tcNgpFHJcOtDJsAg0fDoP2TfpWB+0GufvfclJlZF/xCYh9rbmnl7V9+hH3757n3EychlRq7sgMRyQyLDXPh5QXw0p/hpQWwrcSb8dl6KAyBwmCoGwj5fpArpJ/1kMkn0/9mcq99KguZTPKpTPql5BMly8WftH20bStW4r7KOabT482szx37Yeg/tEenlvtComskfSyXzfDxUw7in2Yu4LdL1/DWQ4aXf7KU1EJGHvXatojkaa8NK2Hji8nnlleSwSCbNiTjeTVtguZG2LYJNr+SLLduh5bm9HM7REvyQmRrS7IckTSrRSvlD8psZnucQ6f3OJGUy4mkAs4+bixf+/USbnx4afcSSSkSDBiefI0uOdBx34hIX5aMopcmi7a1P3bXAso4ppPjzawycv0qf4mKX2EvVMhn+dupB/GF2Yt58vlXOe7AfasdUtdUqvnKzKxrfmqrQj54/IEM6ZfnxoeerXYoZmYV5URSIQPqc1x80jh+vfhlfv/smmqHY2ZWMU4kFfTRt47n4BEDuPT7c1n8omdKNLPXJyeSChpcyPO9jx3PgLocF932BCvWbql2SGZmfc6JpMLG7NOP7350Co3bW7jotidYu3lbtUMyM+tTTiS7wWGjBvGtiybzwrqtXPydOazf6oEazez1w4lkN5kyfijfuOBYFrywnnd8+WHuntfQ+XhcZmZ7CCeS3ehdR43iZ584iQOG9ueKu57i3Ft+z8KV66sdlplZr3isrSpobQ3ufrKB6+57mnVbtvG2w/bj9KNHc9oRIxnSP1/t8MzMAI+1VdMyGXHepAN495GjuPnRZ5k1fyW/eXoVuYw4acJwTjx4GIePHswRowex36BCtcM1M+uUayQ1ICL4U8N6Zi94kf9e8BLPrXntMeHhA+sYP3wA+w0uMGpwgZGD6xk2oJ5BhRyDCnkGFXIMqM9RyGco5LLU5zPU57JkMx7uxMx6p9waiRNJDXp18zYWv7SBxS9uZPGLG1ixdgurNjbx0vpGtm4vMY1vCRLkMiKXyZDLiExGZDMiI8hI6RdI2jHMltJh35PltnK0YyT5ncovec3KJi+nRrPu+/ZFkzlwWP8eneumrT3YvgPqOPHg4Zx48M4jB0cEG5uaeXXzNjY2NrOhcTsbG5vZ3NRMU3MrTdtbks/mVppbWtneGjS3tNLcGrS2Bq0BLZEsR0BrJNsiYsdYvDsvJ2P0tv9jo+SfHhX+eyQ8WrBZj9TlKv9MVUUTiaRpwNeALPCtiLiu3f6TgeuBY4DzI+Luon0HAt8CDiD5NXV6RCwv2v8N4OKIGFjJe6glkhhcyDO44A55M6sdFUtVkrLADcB04EjgAklHtjvseWAG8KMSRXwP+GJEHAFMAVYVlT0J2KcCYZuZWTdVss4zBVgaEcsiYhtwB3Bm8QERsTwi/gS0Fm9PE04uIn6VHrcpIrak+7LAF4FPVzB2MzMrUyUTyRhgRdF6Q7qtHIcC6yT9VNIfJX0xTSAAlwGzIuLFzgqQdKmkuZLmrl69utvBm5lZeSqZSEo9ZFNuj2kOmApcAUwGDgJmSNofOBf4RlcFRMStETEpIiaNGDGizMuamVl3VbKzvYGko7zNWGBlN879Y0QsA5B0L3AC8BIwAViaPmraX9LSiJjQZ1GbmVm3VDKRzAEOkTQeeAE4H/hgN87dV9KIiFgNvB2YGxG/BEa1HSRpk5OImVl1VaxpKyKaSfoz7gcWA3dGxEJJ10o6A0DSZEkNJM1Vt0hamJ7bQtKs9aCkP5M0k32zUrGamVnP+c12MzMryUOkFJG0Gniuh6cPB17pw3AqwTH2jT0hRtgz4nSMfaPaMb4hIrp8WmmvSCS9IWluORm5mhxj39gTYoQ9I07H2Df2hBjBE1uZmVkvOZGYmVmvOJF07dZqB1AGx9g39oQYYc+I0zH2jT0hRveRmJlZ77hGYmZmveJEYmZmveJE0glJ0yQ9I2mppKuqHQ+ApNskrZK0oGjbUEm/krQk/dy3yjEeIOkhSYslLZR0ea3FKakg6QlJT6UxXpNuHy/p8TTGn0iqq1aMRbFm01Gwf1GLMUpaLunPkuZLmptuq5mfdRrPPpLulvR0+u/yLbUUo6TD0u9f29cGSZ+qpRg740TSgTIn5qqG7wDT2m27CngwIg4BHkzXq6kZ+D/ppGQnAJ9Iv3e1FGcT8PaIeBMwEZgm6QTgP4GvpjG+CnysijG2uZxkmKE2tRjjqRExseidh1r6WUMyU+t/R8ThwJtIvp81E2NEPJN+/yYCbwa2ADNrKcZORYS/SnwBbwHuL1q/Gri62nGlsYwDFhStPwOMTpdHA89UO8Z28f4MOK1W4wT6A08Cx5O8RZwr9W+gSrGNJfkF8nbgFyTjztVajMuB4e221czPGhgM/JX04aJajLFdXO8CflvLMbb/co2kY72ZmGt3GxnpRF/p535VjmcHSeOAY4HHqbE40yaj+STTOP8KeBZYF8mAo1AbP/PrSWYDbZtFdBi1F2MAD0iaJ+nSdFst/awPAlYDt6dNhN+SNKDGYix2PvDjdLlWY9yJE0nHejMxlwGSBgL3AJ+KiA3Vjqe9iGiJpClhLMnU0EeUOmz3RvUaSe8FVkXEvOLNJQ6t9r/LkyLiOJJm4E9IOrnK8bSXA44DboqIY4HN1GgTUdrfdQZwV7Vj6Q4nko71ZmKu3e1lSaMB0s9VVY4HSXmSJPLDiPhpurnm4gSIiHXAwyT9OftIapunp9o/85OAMyQtB+4gad66ntqKkYhYmX6uImnXn0Jt/awbgIaIeDxdv5sksdRSjG2mA09GxMvpei3GuAsnko7tmJgr/SvhfGBWlWPqyCzgonT5IpI+iapRMn3lt4HFEfGVol01E6ekEZL2SZf7Ae8k6YB9CDgnPayqMUbE1RExNiLGkfz7+01EfIgailHSAEmD2pZJ2vcXUEM/64h4CVgh6bB00zuARdRQjEUu4LVmLajNGHdV7U6aWv4CTgf+QtJ2/k/VjieN6cfAi8B2kr+0PkbSbv4gsCT9HFrlGN9K0tzyJ2B++nV6LcUJHAP8MY1xAfC5dPtBwBPAUpLmhfpq/8zTuN4G/KLWYkxjeSr9Wtj2/6SWftZpPBOBuenP+15g3xqMsT+wBhhStK2mYuzoy0OkmJlZr7hpy8zMesWJxMzMesWJxMzMesWJxMzMesWJxMzMesWJxKzGSXpb28i/ZrXIicTMzHrFicSsj0i6MJ3jZL6kW9JBITdJ+rKkJyU9KGlEeuxESX+Q9CdJM9vmmZA0QdKv03lSnpR0cFr8wKL5NH6Yjh5gVhOcSMz6gKQjgA+QDGA4EWgBPgQMIBk76TjgEeBf0lO+B3wmIo4B/ly0/YfADZHMk3IiySgGkIyg/CmSuXEOIhmHy6wm5Lo+xMzK8A6SCYnmpJWFfiQD7LUCP0mP+QHwU0lDgH0i4pF0+3eBu9Ixq8ZExEyAiGgESMt7IiIa0vX5JHPSPFb52zLrmhOJWd8Q8N2IuHqnjdJn2x3X2ZhEnTVXNRUtt+D/u1ZD3LRl1jceBM6RtB/smLP8DST/x9pG6v0g8FhErAdelTQ13f5h4JFI5mxpkHRWWka9pP679S7MesB/1Zj1gYhYJOmfSWYKzJCMzvwJkkmUjpI0D1hP0o8CyZDgN6eJYhlwcbr9w8Atkq5Nyzh3N96GWY949F+zCpK0KSIGVjsOs0py05aZmfWKayRmZtYrrpGYmVmvOJGYmVmvOJGYmVmvOJGYmVmvOJGYmVmv/H9yjzGrb06J2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(epochs), train_loss, label='train_loss')\n",
    "plt.plot(range(epochs), test_loss, label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"train and test loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
